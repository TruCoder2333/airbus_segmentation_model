{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97d0b48",
   "metadata": {
    "_cell_guid": "476f6afc-3aaa-4cc7-8060-e8232cd17f8f",
    "_uuid": "80e6d73f-a3d1-4f03-8b5a-7924c1a70fd4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-25T11:40:47.847831Z",
     "iopub.status.busy": "2023-06-25T11:40:47.847439Z",
     "iopub.status.idle": "2023-06-25T12:45:56.718659Z",
     "shell.execute_reply": "2023-06-25T12:45:56.717308Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3908.87894,
     "end_time": "2023-06-25T12:45:56.721723",
     "exception": false,
     "start_time": "2023-06-25T11:40:47.842783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1262 - dice_coef: 0.0110 - binary_accuracy: 0.7249 \n",
      "Epoch 1: val_dice_coef improved from -inf to 0.00897, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 194s 42s/step - loss: 0.1262 - dice_coef: 0.0110 - binary_accuracy: 0.7249 - val_loss: 0.0534 - val_dice_coef: 0.0090 - val_binary_accuracy: 0.9939 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0286 - dice_coef: 0.0053 - binary_accuracy: 0.9853 \n",
      "Epoch 2: val_dice_coef did not improve from 0.00897\n",
      "5/5 [==============================] - 186s 41s/step - loss: 0.0286 - dice_coef: 0.0053 - binary_accuracy: 0.9853 - val_loss: 0.0241 - val_dice_coef: 0.0081 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0154 - dice_coef: 0.0090 - binary_accuracy: 0.9936 \n",
      "Epoch 3: val_dice_coef did not improve from 0.00897\n",
      "5/5 [==============================] - 187s 41s/step - loss: 0.0154 - dice_coef: 0.0090 - binary_accuracy: 0.9936 - val_loss: 0.0124 - val_dice_coef: 0.0073 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0135 - dice_coef: 0.0095 - binary_accuracy: 0.9929 \n",
      "Epoch 4: val_dice_coef improved from 0.00897 to 0.00945, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 185s 41s/step - loss: 0.0135 - dice_coef: 0.0095 - binary_accuracy: 0.9929 - val_loss: 0.0111 - val_dice_coef: 0.0095 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0101 - dice_coef: 0.0112 - binary_accuracy: 0.9934 \n",
      "Epoch 5: val_dice_coef improved from 0.00945 to 0.01136, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 186s 41s/step - loss: 0.0101 - dice_coef: 0.0112 - binary_accuracy: 0.9934 - val_loss: 0.0095 - val_dice_coef: 0.0114 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0097 - dice_coef: 0.0149 - binary_accuracy: 0.9930 \n",
      "Epoch 6: val_dice_coef improved from 0.01136 to 0.01365, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 187s 41s/step - loss: 0.0097 - dice_coef: 0.0149 - binary_accuracy: 0.9930 - val_loss: 0.0087 - val_dice_coef: 0.0137 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0082 - dice_coef: 0.0143 - binary_accuracy: 0.9939 \n",
      "Epoch 7: val_dice_coef improved from 0.01365 to 0.01433, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 186s 41s/step - loss: 0.0082 - dice_coef: 0.0143 - binary_accuracy: 0.9939 - val_loss: 0.0082 - val_dice_coef: 0.0143 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0071 - dice_coef: 0.0133 - binary_accuracy: 0.9945 \n",
      "Epoch 8: val_dice_coef improved from 0.01433 to 0.01462, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 187s 41s/step - loss: 0.0071 - dice_coef: 0.0133 - binary_accuracy: 0.9945 - val_loss: 0.0081 - val_dice_coef: 0.0146 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0079 - dice_coef: 0.0158 - binary_accuracy: 0.9936 \n",
      "Epoch 9: val_dice_coef improved from 0.01462 to 0.01570, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 187s 41s/step - loss: 0.0079 - dice_coef: 0.0158 - binary_accuracy: 0.9936 - val_loss: 0.0077 - val_dice_coef: 0.0157 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0071 - dice_coef: 0.0173 - binary_accuracy: 0.9938 \n",
      "Epoch 10: val_dice_coef improved from 0.01570 to 0.02163, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 187s 42s/step - loss: 0.0071 - dice_coef: 0.0173 - binary_accuracy: 0.9938 - val_loss: 0.0072 - val_dice_coef: 0.0216 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0065 - dice_coef: 0.0217 - binary_accuracy: 0.9943 \n",
      "Epoch 11: val_dice_coef improved from 0.02163 to 0.02843, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 187s 41s/step - loss: 0.0065 - dice_coef: 0.0217 - binary_accuracy: 0.9943 - val_loss: 0.0070 - val_dice_coef: 0.0284 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0067 - dice_coef: 0.0220 - binary_accuracy: 0.9942 \n",
      "Epoch 12: val_dice_coef did not improve from 0.02843\n",
      "5/5 [==============================] - 248s 57s/step - loss: 0.0067 - dice_coef: 0.0220 - binary_accuracy: 0.9942 - val_loss: 0.0067 - val_dice_coef: 0.0270 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0070 - dice_coef: 0.0263 - binary_accuracy: 0.9935 \n",
      "Epoch 13: val_dice_coef did not improve from 0.02843\n",
      "5/5 [==============================] - 189s 42s/step - loss: 0.0070 - dice_coef: 0.0263 - binary_accuracy: 0.9935 - val_loss: 0.0065 - val_dice_coef: 0.0280 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0062 - dice_coef: 0.0270 - binary_accuracy: 0.9942 \n",
      "Epoch 14: val_dice_coef improved from 0.02843 to 0.03348, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 187s 42s/step - loss: 0.0062 - dice_coef: 0.0270 - binary_accuracy: 0.9942 - val_loss: 0.0063 - val_dice_coef: 0.0335 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0063 - dice_coef: 0.0298 - binary_accuracy: 0.9939 \n",
      "Epoch 15: val_dice_coef did not improve from 0.03348\n",
      "5/5 [==============================] - 186s 41s/step - loss: 0.0063 - dice_coef: 0.0298 - binary_accuracy: 0.9939 - val_loss: 0.0060 - val_dice_coef: 0.0327 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0061 - dice_coef: 0.0256 - binary_accuracy: 0.9940 \n",
      "Epoch 16: val_dice_coef did not improve from 0.03348\n",
      "5/5 [==============================] - 187s 41s/step - loss: 0.0061 - dice_coef: 0.0256 - binary_accuracy: 0.9940 - val_loss: 0.0058 - val_dice_coef: 0.0304 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0061 - dice_coef: 0.0257 - binary_accuracy: 0.9937 \n",
      "Epoch 17: val_dice_coef did not improve from 0.03348\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5/5 [==============================] - 190s 42s/step - loss: 0.0061 - dice_coef: 0.0257 - binary_accuracy: 0.9937 - val_loss: 0.0060 - val_dice_coef: 0.0293 - val_binary_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0052 - dice_coef: 0.0254 - binary_accuracy: 0.9944 \n",
      "Epoch 18: val_dice_coef did not improve from 0.03348\n",
      "5/5 [==============================] - 188s 42s/step - loss: 0.0052 - dice_coef: 0.0254 - binary_accuracy: 0.9944 - val_loss: 0.0055 - val_dice_coef: 0.0334 - val_binary_accuracy: 0.9948 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0048 - dice_coef: 0.0241 - binary_accuracy: 0.9949 \n",
      "Epoch 19: val_dice_coef improved from 0.03348 to 0.03418, saving model to /kaggle/working/fullres_model & weights/seg_model_weights.best.hdf5\n",
      "5/5 [==============================] - 247s 56s/step - loss: 0.0048 - dice_coef: 0.0241 - binary_accuracy: 0.9949 - val_loss: 0.0053 - val_dice_coef: 0.0342 - val_binary_accuracy: 0.9948 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0047 - dice_coef: 0.0237 - binary_accuracy: 0.9946 \n",
      "Epoch 20: val_dice_coef did not improve from 0.03418\n",
      "5/5 [==============================] - 186s 41s/step - loss: 0.0047 - dice_coef: 0.0237 - binary_accuracy: 0.9946 - val_loss: 0.0050 - val_dice_coef: 0.0327 - val_binary_accuracy: 0.9948 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.io import imread\n",
    "from data_preparation import train_df, valid_df\n",
    "from rle_and_mask_related import make_image_gen\n",
    "from model import unet, callbacks_list, dg_args\n",
    "import losses\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "train_gen = make_image_gen(train_df, config.BATCH_SIZE, config.IMG_SCALING, config.TRAIN_DIR)\n",
    "train_x, train_y = next(train_gen)\n",
    "valid_x, valid_y = next(make_image_gen(valid_df, config.VALID_IMG_COUNT, config.IMG_SCALING, config.TRAIN_DIR))\n",
    "\n",
    "image_gen = tf.keras.preprocessing.image.ImageDataGenerator(**dg_args)\n",
    "label_gen = tf.keras.preprocessing.image.ImageDataGenerator(**dg_args)\n",
    "\n",
    "def gen_pred(test_dir, img, model):\n",
    "    rgb_path = os.path.join(TEST_DIR,img)\n",
    "    img = cv2.imread(rgb_path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    pred = model.predict(img)\n",
    "    pred = np.squeeze(pred, axis=0)\n",
    "    return cv2.imread(rgb_path), pred\n",
    "\n",
    "def create_aug_gen(in_gen, seed = None):\n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    for in_x, in_y in in_gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n",
    "        g_x = image_gen.flow(255*in_x, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        g_y = label_gen.flow(in_y, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "\n",
    "        yield next(g_x)/255.0, next(g_y)\n",
    "seg_model = unet()\n",
    "\n",
    "aug_gen = create_aug_gen(make_image_gen(train_df, config.BATCH_SIZE, config.IMG_SCALING, config.TRAIN_DIR))\n",
    "seg_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), \n",
    "                  loss = losses.FocalLoss,\n",
    "                  metrics = [losses.dice_coef, \"binary_accuracy\"])\n",
    "\n",
    "seg_model.fit(aug_gen,\n",
    "              steps_per_epoch=config.MAX_TRAIN_STEPS,\n",
    "              epochs= 20,\n",
    "              validation_data=(valid_x, valid_y),\n",
    "              callbacks=callbacks_list,\n",
    "              workers=1)\n",
    "fullres_model = tf.keras.models.Sequential()\n",
    "fullres_model.add(tf.keras.layers.AvgPool2D(config.IMG_SCALING, input_shape = (None, None, 3)))\n",
    "fullres_model.add(seg_model)\n",
    "fullres_model.add(tf.keras.layers.UpSampling2D(config.IMG_SCALING))\n",
    "fullres_model.save('/kaggle/working/fullres_model & weights/fullres_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3924.083499,
   "end_time": "2023-06-25T12:46:00.134937",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-25T11:40:36.051438",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
